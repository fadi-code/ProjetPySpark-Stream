{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a14c940-35a0-42a6-9b68-b1419d3c55b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Création de la session Spark...\n",
      "Configuration des paramètres de connexion JDBC pour PostgreSQL...\n",
      "Lecture des données depuis la table 'Mostodon_BRONZ_bis' dans PostgreSQL...\n",
      "Nombre de lignes lues : 137\n",
      "Suppression des balises HTML dans la colonne 'content'...\n",
      "Filtrage des données en fonction des colonnes 'favourites_count', 'reblogs_count', 'replies_count'...\n",
      "Nombre de lignes après filtrage : 0\n",
      "+---+--------+------------+-------+----------------+-------------+-------------+\n",
      "| id|username|display_name|content|favourites_count|reblogs_count|replies_count|\n",
      "+---+--------+------------+-------+----------------+-------------+-------------+\n",
      "+---+--------+------------+-------+----------------+-------------+-------------+\n",
      "\n",
      "Connexion à PostgreSQL avec psycopg2...\n",
      "Création de la table 'Mostodon_SILVER_bis' si elle n'existe pas déjà...\n",
      "Insertion des données nettoyées dans la table 'Mostodon_SILVER_bis' via Spark JDBC...\n",
      "Insertion terminée.\n",
      "Calcul du nombre total d'entrées dans la table 'Mostodon_SILVER_bis'...\n",
      "Nombre total d'entrées dans la table 'Mostodon_SILVER_bis' : 137\n",
      "Connexion à PostgreSQL fermée.\n",
      "Fermeture de la session Spark...\n",
      "Session Spark fermée.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_replace, col\n",
    "import psycopg2\n",
    "\n",
    "# 1. Créer une session Spark\n",
    "print(\"Création de la session Spark...\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PostgresToSpark\") \\\n",
    "    .config(\"spark.jars\", \"/usr/local/spark/jars/postgresql-42.3.9.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2. Configurer les paramètres de connexion\n",
    "print(\"Configuration des paramètres de connexion JDBC pour PostgreSQL...\")\n",
    "jdbc_url = \"jdbc:postgresql://postgres:5432/DB_Mastodon\"  # Remplacez 'postgres' et 'DB_Mastodon' par vos valeurs\n",
    "properties = {\n",
    "    \"user\": \"fadi\",        # Votre nom d'utilisateur PostgreSQL\n",
    "    \"password\": \"fadi\",    # Votre mot de passe PostgreSQL\n",
    "    \"driver\": \"org.postgresql.Driver\"  # Driver JDBC PostgreSQL\n",
    "}\n",
    "\n",
    "# 3. Lire les données depuis PostgreSQL\n",
    "print(\"Lecture des données depuis la table 'Mostodon_BRONZ_bis' dans PostgreSQL...\")\n",
    "df = spark.read.jdbc(url=jdbc_url, table=\"Mostodon_BRONZ_bis\", properties=properties)\n",
    "print(f\"Nombre de lignes lues : {df.count()}\")\n",
    "\n",
    "# 4. Nettoyer la colonne 'content' des balises HTML\n",
    "print(\"Suppression des balises HTML dans la colonne 'content'...\")\n",
    "df_cleaned = df.withColumn(\"content\", regexp_replace(col(\"content\"), \"<[^>]+>\", \"\"))\n",
    "\n",
    "# 5. Filtrer les lignes avec certaines conditions\n",
    "print(\"Filtrage des données en fonction des colonnes 'favourites_count', 'reblogs_count', 'replies_count'...\")\n",
    "df_max_values = df_cleaned.filter(\n",
    "    (col(\"favourites_count\") > 1) & \n",
    "    (col(\"reblogs_count\") > 1) & \n",
    "    (col(\"replies_count\") > 1)\n",
    ")\n",
    "print(f\"Nombre de lignes après filtrage : {df_max_values.count()}\")\n",
    "\n",
    "# 6. Afficher les résultats filtrés\n",
    "df_max_values.show()\n",
    "\n",
    "# 7. Connexion à PostgreSQL via psycopg2\n",
    "print(\"Connexion à PostgreSQL avec psycopg2...\")\n",
    "conn = psycopg2.connect(\n",
    "    host=\"postgres\",  # Changez ceci si nécessaire\n",
    "    database=\"DB_Mastodon\",\n",
    "    user=\"fadi\",\n",
    "    password=\"fadi\"\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# 8. Création de la table 'Mostodon_SILVER_bis' si elle n'existe pas déjà\n",
    "print(\"Création de la table 'Mostodon_SILVER_bis' si elle n'existe pas déjà...\")\n",
    "create_table_query = '''\n",
    "CREATE TABLE IF NOT EXISTS Mostodon_SILVER_bis (\n",
    "    id BIGINT PRIMARY KEY,\n",
    "    username TEXT NOT NULL,\n",
    "    display_name TEXT NOT NULL,\n",
    "    content TEXT NOT NULL,\n",
    "    favourites_count INT NOT NULL,\n",
    "    reblogs_count INT NOT NULL,\n",
    "    replies_count INT NOT NULL\n",
    ");\n",
    "'''\n",
    "cursor.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "# 9. Lire les IDs existants dans la table Mostodon_SILVER_bis\n",
    "existing_ids = spark.read.jdbc(url=jdbc_url, table=\"Mostodon_SILVER_bis\", properties=properties) \\\n",
    "    .select(\"id\") \\\n",
    "    .rdd.flatMap(lambda x: x).collect()  # Collecter les IDs dans une liste\n",
    "\n",
    "# 10. Filtrer le DataFrame pour exclure les enregistrements existants\n",
    "df_filtered = df_cleaned.filter(~col(\"id\").isin(existing_ids))\n",
    "\n",
    "# 11. Insertion des données filtrées dans la table Mostodon_SILVER_bis\n",
    "print(\"Insertion des données nettoyées dans la table 'Mostodon_SILVER_bis' via Spark JDBC...\")\n",
    "df_filtered.write.jdbc(url=jdbc_url, table=\"Mostodon_SILVER_bis\", mode=\"append\", properties=properties)\n",
    "print(\"Insertion terminée.\")\n",
    "\n",
    "# 12. Compter le nombre total d'entrées dans la table 'Mostodon_SILVER_bis'\n",
    "print(\"Calcul du nombre total d'entrées dans la table 'Mostodon_SILVER_bis'...\")\n",
    "cursor.execute(\"SELECT COUNT(*) FROM Mostodon_SILVER_bis;\")\n",
    "count = cursor.fetchone()[0]  # Récupérer le résultat du COUNT\n",
    "print(f\"Nombre total d'entrées dans la table 'Mostodon_SILVER_bis' : {count}\")\n",
    "\n",
    "# 13. Fermer le curseur et la connexion PostgreSQL\n",
    "cursor.close()\n",
    "conn.close()\n",
    "print(\"Connexion à PostgreSQL fermée.\")\n",
    "\n",
    "# 14. Fermer la session Spark\n",
    "print(\"Fermeture de la session Spark...\")\n",
    "spark.stop()\n",
    "print(\"Session Spark fermée.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01276e21-1782-45c8-8433-729a587ccec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbfdbea-b49b-4ae3-8da2-d76daf22d853",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
