{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a14c940-35a0-42a6-9b68-b1419d3c55b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Création de la session Spark...\n",
      "Configuration des paramètres de connexion JDBC pour PostgreSQL...\n",
      "Lecture des données depuis la table 'toots_BRONZ' dans PostgreSQL...\n",
      "Nombre de lignes lues : 4927\n",
      "Suppression des balises HTML dans la colonne 'content'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py\", line 475, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "RuntimeError: reentrant call inside <_io.BufferedReader name=52>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py\", line 503, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py\", line 475, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/conda/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 292, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 1195, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 111, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o617.sc\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py\", line 503, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling z:org.apache.spark.sql.functions.regexp_replace",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# 4. Nettoyer la colonne 'content' des balises HTML\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuppression des balises HTML dans la colonne \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m df_cleaned \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mregexp_replace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<[^>]+>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# 5. Filtrer les lignes avec certaines conditions\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiltrage des données en fonction des colonnes \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfavourites_count\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreblogs_count\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplies_count\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/functions.py:3019\u001b[0m, in \u001b[0;36mregexp_replace\u001b[0;34m(str, pattern, replacement)\u001b[0m\n\u001b[1;32m   3008\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Replace all substrings of the specified string value that match regexp with rep.\u001b[39;00m\n\u001b[1;32m   3009\u001b[0m \n\u001b[1;32m   3010\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.5.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3016\u001b[0m \u001b[38;5;124;03m[Row(d='-----')]\u001b[39;00m\n\u001b[1;32m   3017\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3018\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n\u001b[0;32m-> 3019\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregexp_replace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_java_column\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplacement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3020\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling z:org.apache.spark.sql.functions.regexp_replace"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_replace, col\n",
    "import psycopg2\n",
    "\n",
    "# 1. Créer une session Spark\n",
    "print(\"Création de la session Spark...\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PostgresToSpark\") \\\n",
    "    .config(\"spark.jars\", \"/usr/local/spark/jars/postgresql-42.3.9.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2. Configurer les paramètres de connexion\n",
    "print(\"Configuration des paramètres de connexion JDBC pour PostgreSQL...\")\n",
    "jdbc_url = \"jdbc:postgresql://postgres:5432/DB_Mastodon\"  # Remplacez 'postgres' et 'DB_Mastodon' par vos valeurs\n",
    "properties = {\n",
    "    \"user\": \"fadi\",        # Votre nom d'utilisateur PostgreSQL\n",
    "    \"password\": \"fadi\",    # Votre mot de passe PostgreSQL\n",
    "    \"driver\": \"org.postgresql.Driver\"  # Driver JDBC PostgreSQL\n",
    "}\n",
    "\n",
    "# 3. Lire les données depuis PostgreSQL\n",
    "print(\"Lecture des données depuis la table 'Mosotodon_BRONZ' dans PostgreSQL...\")\n",
    "df = spark.read.jdbc(url=jdbc_url, table=\"Mostodon_BRONZ\", properties=properties)\n",
    "print(f\"Nombre de lignes lues : {df.count()}\")\n",
    "\n",
    "# 4. Nettoyer la colonne 'content' des balises HTML\n",
    "print(\"Suppression des balises HTML dans la colonne 'content'...\")\n",
    "df_cleaned = df.withColumn(\"content\", regexp_replace(col(\"content\"), \"<[^>]+>\", \"\"))\n",
    "\n",
    "# 5. Filtrer les lignes avec certaines conditions\n",
    "print(\"Filtrage des données en fonction des colonnes 'favourites_count', 'reblogs_count', 'replies_count'...\")\n",
    "df_max_values = df_cleaned.filter(\n",
    "    (col(\"favourites_count\") > 1) | \n",
    "    (col(\"reblogs_count\") > 1) | \n",
    "    (col(\"replies_count\") > 1)\n",
    ")\n",
    "print(f\"Nombre de lignes après filtrage : {df_max_values.count()}\")\n",
    "\n",
    "# 6. Afficher les résultats filtrés\n",
    "df_max_values.show()\n",
    "\n",
    "# 7. Connexion à PostgreSQL via psycopg2\n",
    "print(\"Connexion à PostgreSQL avec psycopg2...\")\n",
    "conn = psycopg2.connect(\n",
    "    host=\"postgres\",  # Changez ceci si nécessaire\n",
    "    database=\"DB_Mastodon\",\n",
    "    user=\"fadi\",\n",
    "    password=\"fadi\"\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# 8. Création de la table 'Toots_SILVER' si elle n'existe pas déjà\n",
    "print(\"Création de la table 'Mostodon_SILVER' si elle n'existe pas déjà...\")\n",
    "create_table_query = '''\n",
    "CREATE TABLE IF NOT EXISTS Mostodon_SILVER (\n",
    "    id BIGINT PRIMARY KEY,\n",
    "    username TEXT NOT NULL,\n",
    "    display_name TEXT NOT NULL,\n",
    "    content TEXT NOT NULL,\n",
    "    created_at TIMESTAMP NOT NULL,\n",
    "    favourites_count INT NOT NULL,\n",
    "    reblogs_count INT NOT NULL,\n",
    "    replies_count INT NOT NULL\n",
    ");\n",
    "'''\n",
    "cursor.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "# 9. Insertion des données nettoyées dans la table 'Toots_SILVER'\n",
    "print(\"Insertion des données nettoyées dans la table 'Mostodon_SILVER' via Spark JDBC...\")\n",
    "df_cleaned.write.jdbc(url=jdbc_url, table=\"Mostodon_SILVER\", mode=\"append\", properties=properties)\n",
    "print(\"Insertion terminée.\")\n",
    "\n",
    "# 10. Compter le nombre total d'entrées dans la table 'Toots_SILVER'\n",
    "print(\"Calcul du nombre total d'entrées dans la table 'Toots_SILVER'...\")\n",
    "cursor.execute(\"SELECT COUNT(*) FROM Mostodon_SILVER;\")\n",
    "count = cursor.fetchone()[0]  # Récupérer le résultat du COUNT\n",
    "print(f\"Nombre total d'entrées dans la table 'Mostodon_SILVER' : {count}\")\n",
    "\n",
    "# 11. Fermer le curseur et la connexion PostgreSQL\n",
    "cursor.close()\n",
    "conn.close()\n",
    "print(\"Connexion à PostgreSQL fermée.\")\n",
    "\n",
    "# 12. Fermer la session Spark\n",
    "print(\"Fermeture de la session Spark...\")\n",
    "spark.stop()\n",
    "print(\"Session Spark fermée.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01276e21-1782-45c8-8433-729a587ccec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbfdbea-b49b-4ae3-8da2-d76daf22d853",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
